{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12  8  9 10 11 10  3  0  0  0]\n",
      " [12  5 10 12  4 10 11  7  6  3]\n",
      " [11  9  4  5  8  6  4 11 12  3]\n",
      " [ 8  9  9  3  0  0  0  0  0  0]]\n",
      "[ 8 10 10  5]\n",
      "[[7 5 5 6 6 7 6 6 7 7 6 6 7 3 0]\n",
      " [7 4 6 6 7 7 4 6 6 7 7 5 5 4 3]\n",
      " [7 6 6 7 4 4 5 5 4 4 7 7 3 0 0]\n",
      " [5 5 6 6 7 6 6 7 3 0 0 0 0 0 0]]\n",
      "[15 15 14 10]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.python.client import device_lib#获取cpu，gpu操作包\n",
    "from word_sequence import WordSequence\n",
    "\n",
    "VOCAB_SIZE_THRESHOLD_CPU = 50000\n",
    "\n",
    "'''获取当前GPU信息'''\n",
    "def _get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "'''根据输入输出的字典大小来选择，是在CPU上embedding还是在GPU上进行embedding'''\n",
    "def _get_embed_device(vocab_size):#超过某个临界值选择在cpu处理\n",
    "    gpus = _get_available_gpus()\n",
    "    if not gpus or vocab_size > VOCAB_SIZE_THRESHOLD_CPU:\n",
    "        return \"/cpu:0\"\n",
    "    return \"/gpu:0\"\n",
    "\n",
    "\n",
    "def transform_sentence(sentence, ws, max_len=None, add_end=False):#句子，word_sequence，最大长度\n",
    "    \"\"\" 单独的句子的转换 \"\"\"\n",
    "    encoded = ws.transform(\n",
    "        sentence,\n",
    "        max_len= max_len if max_len is not None else len(sentence))\n",
    "    encoded_len = len(sentence) + (1 if add_end else 0)\n",
    "    if encoded_len > len(encoded):\n",
    "        encoded_len = len(encoded)\n",
    "    #[4, 4, 5, 6]\n",
    "    return encoded, encoded_len\n",
    "\n",
    "\n",
    "def batch_flow(data, ws, batch_size, raw=False, add_end=True):#ws可以使一个WordSequence的对象也可是对象的列表或元组\n",
    "                                                                #data同理\n",
    "    \"\"\"\n",
    "    从数据中随机去生成batch_size的数据，然后给转换后输出出去\n",
    "    raw:是否返回原始对象，如果为True，假设结果ret， 那么len(ret) == len(data) * 3\n",
    "        如果为false，那么len(ret) == len(data） * 2\n",
    "    Q = (q1, q2, q3 ... qn)\n",
    "    A = (a1, a2, a3 ... an)\n",
    "    len(Q) == len(A)\n",
    "    batch_flow([Q, A], ws, batch_size = 32)\n",
    "    raw = False:\n",
    "    next(generator) == q_i_encoded, q_i_len, a_i_encoded, a_i_len\n",
    "    raw = True:\n",
    "    next(generator) == q_i_encoded, q_i_len, q_i, a_i_encoded, a_i_len, a_i\n",
    "    \"\"\"\n",
    "    #ws数量要和data数量要保持一致（多个）,len(data) == len(ws)\n",
    "    all_data = list(zip(*data))\n",
    "    #[(['1', '2'], ['a', 'b']), (['2', '3', '4'], ['b', 'c', 'd']), (['1', '3', '4'], ['a', 'c', 'd'])]\n",
    "    if isinstance(ws, (list, tuple)):#ws是否是list或tuple\n",
    "        assert len(ws) == len(data), 'ws的长度必须等于data的长度 if ws 是一个list or tuple'\n",
    "\n",
    "    if isinstance(add_end, bool):\n",
    "        add_end = [add_end] * len(data)\n",
    "    else:\n",
    "        assert(isinstance(add_end, (list, tuple))), 'add_end不是boolean，就应该是一个list(tuple) of boolean'\n",
    "        assert len(add_end) == len(data), '如果add_end 是list(tuple)，那么add_end的长度应该和输入数据的长度一致'\n",
    "\n",
    "    mul = 2\n",
    "    if raw:\n",
    "        mul = 3\n",
    "\n",
    "    while True:\n",
    "        data_batch = random.sample(all_data, batch_size) #在all_data数据中随机抽取生成batch_size个数据\n",
    "        batches = [[] for i in range(len(data) * mul)]\n",
    "\n",
    "        max_lens = []#求所有数据中每句句子的长度\n",
    "        for j in range(len(data)):#len（data）就是2，为[x,y]      \n",
    "            max_len = max([len(x[j]) if hasattr(x[j], '__len__') else 0 for x in data_batch]) + (1 if add_end[j] else 0)\n",
    "            #data_batch为如下格式[(['1', '2'], ['a', 'b']), (['2', '3', '4'], ['b', 'c', 'd']), (['1', '3', '4'], ['a', 'c', 'd'])]\n",
    "            #即在all_data中选一些元素出来\n",
    "            #判断在这一个batch中，最长的数字1,2以及最长的字母对应abc\n",
    "            max_lens.append(max_len)#判断对象是否有某种属性/方法\n",
    "\n",
    "        for d in data_batch:\n",
    "            for j in range(len(data)):\n",
    "                if isinstance(ws, (list, tuple)):\n",
    "                    w = ws[j]\n",
    "                else:\n",
    "                    w = ws\n",
    "\n",
    "                #添加结束标记（结尾）\n",
    "                line = d[j]\n",
    "                if add_end[j] and isinstance(line, (tuple, list)):\n",
    "                    line = list(line) + [WordSequence.END_TAG]\n",
    "                if w is not None:\n",
    "                    x, xl = transform_sentence(line, w, max_lens[j], add_end[j])#data，ws可以为不同组输入，如输入3组data，分别对应三组ws\n",
    "                    batches[j * mul].append(x) #最后生成batches时 batches[0,1]分别为第一组数据的翻译，长度。[2,3]为第二组数据的\n",
    "                    batches[j * mul + 1].append(xl)#如果需要raw，还要返回原始数据,此时batches[0,1,2]对应第一组数据的转换，长度，原始数据\n",
    "                else:                             #3,4,5对应第二组数据的\n",
    "                    batches[j * mul].append(line)\n",
    "                    batches[j * mul + 1].append(line)\n",
    "                if raw:\n",
    "                    batches[j * mul + 2].append(line)\n",
    "        batches = [np.asarray(x) for x in batches]\n",
    "        yield batches\n",
    "\n",
    "def batch_flow_bucket(data, ws, batch_size, raw=False, add_end=True,#分块切分数据\n",
    "                      n_bucket=5, bucket_ind=1, debug=False):#n_bucket把数据分成五分，bucket_ind切分的维度\n",
    "\n",
    "    all_data = list(zip(*data))\n",
    "    #[(['1', '2'], ['a', 'b']), (['2', '3', '4'], ['b', 'c', 'd']), (['1', '3', '4'], ['a', 'c', 'd'])]\n",
    "    lengths = sorted(list(set([len(x[bucket_ind]) for x in all_data])))#通过排序得到数据长度,是排序后的所有x[bucked_ind]的集合的列表\n",
    "    if n_bucket > len(lengths): #如[2,3]：x y中的单词长度只有2和3这两种\n",
    "        n_bucket = len(lengths)\n",
    "\n",
    "    splits = np.array(lengths)[(np.linspace(0, 1, 5, endpoint=False) * len(lengths)).astype(int)].tolist()\n",
    "                                                    #结尾添加top标记\n",
    "\n",
    "    splits += [np.inf] #np.inf无限大的正整数\n",
    "    if debug:\n",
    "        print(splits)\n",
    "\n",
    "    ind_data = {}\n",
    "    for x in all_data:\n",
    "        l = len(x[bucket_ind])\n",
    "        for ind, s in enumerate(splits[:-1]):\n",
    "            if l >= s and l <= splits[ind + 1]:\n",
    "                if ind not in ind_data:\n",
    "                    ind_data[ind] = []\n",
    "                ind_data[ind].append(x)\n",
    "                break\n",
    "\n",
    "    inds = sorted(list(ind_data.keys()))\n",
    "    ind_p = [len(ind_data[x]) / len(all_data) for x in inds]#分布的概率\n",
    "    if debug:\n",
    "        print(np.sum(ind_p), ind_p)\n",
    "\n",
    "    if isinstance(ws, (list, tuple)):\n",
    "        assert len(ws) == len(data), \"len(ws) 必须等于len(data)，ws是list或者是tuple\"\n",
    "\n",
    "    if isinstance(add_end, bool):\n",
    "        add_end = [add_end] * len(data)\n",
    "    else:\n",
    "        assert(isinstance(add_end, (list, tuple))), \"add_end 不是 boolean，就应该是一个list(tuple) of boolean\"\n",
    "        assert len(add_end) == len(data), \"如果add_end 是list(tuple)，那么add_end的长度应该和输入数据长度是一致的\"\n",
    "\n",
    "    mul = 2\n",
    "    if raw:\n",
    "        mul = 3\n",
    "\n",
    "    while True:\n",
    "        choice_ind = np.random.choice(inds, p=ind_p)\n",
    "        if debug:\n",
    "            print('choice_ind', choice_ind)\n",
    "        data_batch = random.sample(ind_data[choice_ind], batch_size)\n",
    "        batches = [[] for i in range(len(data) * mul)]\n",
    "\n",
    "        max_lens = []\n",
    "        for j in range(len(data)):\n",
    "            max_len = max([\n",
    "                len(x[j]) if hasattr(x[j], '__len__') else 0\n",
    "                for x in data_batch\n",
    "            ]) + (1 if add_end[j] else 0)\n",
    "\n",
    "            max_lens.append(max_len)\n",
    "\n",
    "        for d in data_batch:\n",
    "            for j in range(len(data)):\n",
    "                if isinstance(ws, (list, tuple)):\n",
    "                    w = ws[j]\n",
    "                else:\n",
    "                    w = ws\n",
    "\n",
    "                #添加结尾\n",
    "                line = d[j]\n",
    "                if add_end[j] and isinstance(line, (tuple, list)):\n",
    "                    line = list(line) + [WordSequence.END_TAG]\n",
    "\n",
    "                if w is not None:\n",
    "                    x, xl = transform_sentence(line, w, max_lens[j], add_end[j])\n",
    "                    batches[j * mul].append(x)\n",
    "                    batches[j * mul + 1].append(xl)\n",
    "                else:\n",
    "                    batches[j * mul].append(line)\n",
    "                    batches[j * mul + 1].append(line)\n",
    "                if raw:\n",
    "                    batches[j * mul + 2].append(line)\n",
    "        batches = [np.asarray(x) for x in batches]\n",
    "\n",
    "        yield batches\n",
    "\n",
    "def test_batch_flow():\n",
    "    from fake_data import generate\n",
    "    x_data, y_data, ws_input, ws_target = generate(size=100)\n",
    "    flow = batch_flow([x_data, y_data], [ws_input, ws_target], 4)\n",
    "    x, xl, y, yl = next(flow)\n",
    "    #x = next(flow)\n",
    "    print(x)\n",
    "    print(xl)\n",
    "    print(y)\n",
    "    print(yl)\n",
    "    #print(x.shape, y.shape, xl.shape, yl.shape)\n",
    "\n",
    "def test_batch_flow_bucket():\n",
    "    from fake_data import generate\n",
    "    x_data, y_data, ws_input, ws_target = generate(size=100)\n",
    "    flow = batch_flow_bucket([x_data, y_data], [ws_input, ws_target], 4, debug=True)\n",
    "    for _ in range(10):\n",
    "        x, xl, y, yl = next(flow)\n",
    "        print(x.shape, y.shape, xl.shape, yl.shape)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # size = 300000\n",
    "    # print(_get_embed_device(size))\n",
    "    #test_batch_flow_bucket()\n",
    "    test_batch_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['1', '2'], ['a', 'b']), (['2', '3', '4'], ['b', 'c', 'd']), (['1', '3', '4'], ['a', 'c', 'd'])]\n",
      "[2, 3]\n"
     ]
    }
   ],
   "source": [
    "x = [['1','2'],\n",
    "    ['2','3','4'],\n",
    "    ['1','3','4']]\n",
    "\n",
    "y = [['a','b'],\n",
    "    ['b','c','d'],\n",
    "    ['a','c','d']]\n",
    "\n",
    "data  = [x,y]\n",
    "\n",
    "all_data = list(zip(*data))\n",
    "print(all_data)\n",
    "\n",
    "lengths = sorted(list(set([len(x[1]) for x in all_data])))\n",
    "print(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
